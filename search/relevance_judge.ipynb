{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ddfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from agents import Agent, AgentOutputSchema \n",
    "import time\n",
    "import asyncio\n",
    "from agents.run import Runner\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declares the expected schema for judge output (scoring per criterion) and overall evaluation instructions.\n",
    "# The comments and string below guide the AI judge on how to systematically score and explain results.\n",
    "# - JudgeReport and Criteria specify the format for output and feedback.\n",
    "# - Instructions detail how to match the rubric, score each API on every criterion, and add a latency score.\n",
    "\n",
    "import json\n",
    "\n",
    "def _msg(payload: dict) -> list:\n",
    "    return [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"input_text\", \"text\": json.dumps(payload)}]\n",
    "    }]\n",
    "\n",
    "class Criteria(BaseModel):\n",
    "    criteria_name: str \n",
    "    score: float\n",
    "    notes: str \n",
    "\n",
    "class JudgeReport(BaseModel):\n",
    "    query: str\n",
    "    criteria: List[Criteria]\n",
    "\n",
    "\n",
    "judge_agent = Agent(\n",
    "    name=\"ProductSearchQualityJudge\",\n",
    "    model=\"gpt-5\", \n",
    "    instructions=(\n",
    "        \"You are an objective evaluator for shopping/search APIs.\\n\"\n",
    "        \"Do NOT browse the web. Use only the inputs provided.\\n\\n\"\n",
    "\n",
    "        \"Inputs:\\n\"\n",
    "        \"- query: the user's search query.\\n\"\n",
    "        \"- apis: array of { name, latency_sec (float), results: [ {title, url, price?} ] } per API.\\n\"\n",
    "        \"- rubric_json: a JSON rubric defining detailed evaluation criteria for multiple queries.\\n\\n\"\n",
    "\n",
    "        \"Your job:\\n\"\n",
    "        \"1. Identify the rubric in rubric_json whose 'query' best matches the given query (case-insensitive exact match preferred).\\n\"\n",
    "        \"2. For that rubric, evaluate each API’s results across all five criteria listed under 'criteria'.\\n\"\n",
    "        \"3. For each criterion, assign a numeric score in [1,5] using the definitions in 'score_definitions'. Interpret descriptions precisely and grade deterministically. 1 is the lowest and 5 is the highest\\n\"\n",
    "        \"4. Add a latency adjustment: latency_score = 1 / (1 + latency_sec/2), clipped to [0,1]\"\n",
    "        \"5. Round all numeric scores to 2 decimals\"\n",
    "\n",
    "        \"Scoring Details:\\n\"\n",
    "        \"- For each result:\\n\"\n",
    "        \"  • Analyze its results (title, url, price if given) to infer how well each criterion is met.\\n\"\n",
    "        \"  • If results are missing or malformed, score conservatively (1 or 2).\\n\"\n",
    "        \"  • Be consistent and deterministic.\\n\\n\"\n",
    "\n",
    "        \"Rules & Edge Cases:\\n\"\n",
    "        \"1) Use ONLY the rubric_json and the API results. Do NOT infer external data.\\n\"\n",
    "        \"2) If query not found in rubric_json, return an empty per_api list and winner='None'.\\n\"\n",
    "        \"3) 'notes' must summarize key strengths or weaknesses from the rubric perspective.\\n\"\n",
    "    ),\n",
    "    tools=[],  # pure rubric-based judging\n",
    "    output_type=AgentOutputSchema(JudgeReport, strict_json_schema=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77337b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we read the rubric from evals. The evals consist of the query as well as how the answer should be graded.\n",
    "\n",
    "with open(\"evals/product_search_rubric.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        rubric_data = json.load(f)\n",
    "\n",
    "evals = []\n",
    "for item in rubric_data.get(\"rubrics\", []):\n",
    "    evals.append({\n",
    "        \"query\": item.get(\"query\"),\n",
    "        \"criteria\": item.get(\"criteria\")\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3875d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calls the script to run either exa search or parallel search in async\n",
    "\n",
    "async def run_search(script_name, query):\n",
    "    \"\"\"Run a search script and return results with timing\"\"\"\n",
    "    start_time = time.time()\n",
    "    script_path = f\"/Users/karthikapurushothaman/projects/search/{script_name}\"\n",
    "    process = await asyncio.create_subprocess_exec(\n",
    "        \"/Users/karthikapurushothaman/projects/search/.venv/bin/python\",\n",
    "        script_path, \"-q\", query, \"-f\", \"json\",\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE\n",
    "    )\n",
    "    stdout, stderr = await process.communicate()\n",
    "    latency = time.time() - start_time\n",
    "    results = json.loads(stdout.decode())\n",
    "    return results, latency\n",
    "\n",
    "\n",
    "# calls the judge to evaluate the results\n",
    "async def evaluate_result(api_name, query, idx, item, latency, rubric_data):\n",
    "    \"\"\"Evaluate a single result using the judge agent\"\"\"\n",
    "    formatted = [{\n",
    "        'title': item.get('title', ''),\n",
    "        'url': item.get('url', ''),\n",
    "        'price': item.get('price')\n",
    "    }]\n",
    "    \n",
    "    payload = {\n",
    "        'query': query,\n",
    "        'apis': [{\n",
    "            'name': api_name,\n",
    "            'latency_sec': latency,\n",
    "            'results': formatted\n",
    "        }],\n",
    "        'rubric_json': rubric_data\n",
    "    }\n",
    "    \n",
    "    result = await Runner.run(judge_agent, _msg(payload))\n",
    "    judge_output = result.final_output_as(JudgeReport)\n",
    "    return (api_name, query, idx, item, judge_output)\n",
    "\n",
    "\n",
    "# This iterates over all the evals, gets the query, calls the script to get the \n",
    "# results (this includes both exa and parallel), \n",
    "# and then feeds in the criteria to the judge so that the judge can score it.\n",
    "\n",
    "for eval_type in evals:\n",
    "    query = eval_type[\"query\"]\n",
    "    evaluation_criteria = eval_type[\"criteria\"]\n",
    "\n",
    "    print(f\"Searching for: '{query}'\")\n",
    "    \n",
    "    # Run both searches in parallel\n",
    "    exa_task = run_search(\"exa_search.py\", query)\n",
    "    pws_task = run_search(\"pws_search.py\", query)\n",
    "    \n",
    "    (exa_results, exa_latency), (pws_results, pws_search) = await asyncio.gather(exa_task, pws_task)\n",
    "    \n",
    "    print(f\"✓ EXA completed in {exa_latency:.2f}s\")\n",
    "    print(f\"✓ PWS completed in {pws_search:.2f}s\")\n",
    "\n",
    "    # Prepare all evaluation tasks (both EXA and PWS in parallel)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EVALUATING RESULTS IN PARALLEL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    eval_tasks = []\n",
    "    \n",
    "    # Add EXA evaluation tasks\n",
    "    for idx, item in enumerate(exa_results.get('all_results', [])[:10], 1):\n",
    "        task = evaluate_result('EXA', query, idx, item, exa_latency, rubric_data)\n",
    "        eval_tasks.append(task)\n",
    "    \n",
    "    # Add PWS evaluation tasks\n",
    "    for idx, item in enumerate(pws_results.get('output', {}).get('matched_products', [])[:10], 1):\n",
    "        task = evaluate_result('PWS', query, idx, item, pws_search, rubric_data)\n",
    "        eval_tasks.append(task)\n",
    "    \n",
    "    # Run all evaluations in parallel\n",
    "    print(f\"Running {len(eval_tasks)} evaluations in parallel...\")\n",
    "    all_evaluations = await asyncio.gather(*eval_tasks)\n",
    "    time.sleep(10)\n",
    "    print(f\"✓ Completed {len(all_evaluations)} evaluations\")\n",
    "    # Prepare EXA data for Excel\n",
    "    exa_data = []\n",
    "\n",
    "    for api, query, idx, item, judge_output in all_evaluations:\n",
    "        if api == 'EXA':\n",
    "            row = {\n",
    "                'Result #': idx,\n",
    "                'Title': item.get('title', 'N/A'),\n",
    "                'URL': item.get('url', 'N/A'),\n",
    "                'Price': item.get('price', 'N/A'),\n",
    "            }\n",
    "            # Add each criterion score and notes (sanitize for Excel)\n",
    "            for criterion in judge_output.criteria:\n",
    "                # Remove ALL illegal Excel characters including control chars\n",
    "                clean_name = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F≤≥:()]', '', criterion.criteria_name).strip()\n",
    "                row[f\"{clean_name} - Score\"] = criterion.score\n",
    "                row[f\"{clean_name} - Reasoning\"] = criterion.notes\n",
    "            \n",
    "            exa_data.append(row)\n",
    "\n",
    "    # Prepare PWS data for Excel\n",
    "    pws_data = []\n",
    "    for api, query, idx, item, judge_output in all_evaluations:\n",
    "        if api == 'PWS':\n",
    "            row = {\n",
    "                'Result #': idx,\n",
    "                'Title': item.get('title', 'N/A'),\n",
    "                'URL': item.get('url', 'N/A'),\n",
    "                'Price': item.get('price', 'N/A'),\n",
    "            }\n",
    "            # Add each criterion score and notes (sanitize for Excel)\n",
    "            for criterion in judge_output.criteria:\n",
    "                # Remove ALL illegal Excel characters including control chars\n",
    "                clean_name = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F≤≥:()]', '', criterion.criteria_name).strip()\n",
    "                row[f\"{clean_name} - Score\"] = criterion.score\n",
    "                row[f\"{clean_name} - Reasoning\"] = criterion.notes\n",
    "            \n",
    "            pws_data.append(row)\n",
    "\n",
    "    # Create DataFrames\n",
    "    exa_df = pd.DataFrame(exa_data)\n",
    "    pws_df = pd.DataFrame(pws_data)\n",
    "\n",
    "    # Save to Excel files\n",
    "    exa_filename = f'exa_evaluation_report_{query.replace(\" \", \"_\")}.xlsx'\n",
    "    pws_filename = f'pws_evaluation_report_{query.replace(\" \", \"_\")}.xlsx'\n",
    "\n",
    "    exa_df.to_excel(exa_filename, index=False, engine='openpyxl')\n",
    "    pws_df.to_excel(pws_filename, index=False, engine='openpyxl')\n",
    "\n",
    "    print(f\"Created EXA report: {exa_filename} ({len(exa_df)} results)\")\n",
    "    print(f\"Created PWS report: {pws_filename} ({len(pws_df)} results)\")\n",
    "\n",
    "            \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (exa_venv)",
   "language": "python",
   "name": "exa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
