{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b6ddfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from agents import Agent, AgentOutputSchema \n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51d9316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def _msg(payload: dict) -> list:\n",
    "    return [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"input_text\", \"text\": json.dumps(payload)}]\n",
    "    }]\n",
    "\n",
    "class Criteria(BaseModel):\n",
    "    criteria_name: str \n",
    "    score: float\n",
    "    notes: str \n",
    "\n",
    "class JudgeReport(BaseModel):\n",
    "    query: str\n",
    "    criteria: List[Criteria]\n",
    "\n",
    "\n",
    "judge_agent = Agent(\n",
    "    name=\"ProductSearchQualityJudge\",\n",
    "    model=\"gpt-5\", \n",
    "    instructions=(\n",
    "        \"You are an objective evaluator for shopping/search APIs.\\n\"\n",
    "        \"Do NOT browse the web. Use only the inputs provided.\\n\\n\"\n",
    "\n",
    "        \"Inputs:\\n\"\n",
    "        \"- query: the user's search query.\\n\"\n",
    "        \"- apis: array of { name, latency_sec (float), results: [ {title, url, price?} ] } per API.\\n\"\n",
    "        \"- rubric_json: a JSON rubric defining detailed evaluation criteria for multiple queries.\\n\\n\"\n",
    "\n",
    "        \"Your job:\\n\"\n",
    "        \"1. Identify the rubric in rubric_json whose 'query' best matches the given query (case-insensitive exact match preferred).\\n\"\n",
    "        \"2. For that rubric, evaluate each API’s results across all five criteria listed under 'criteria'.\\n\"\n",
    "        \"3. For each criterion, assign a numeric score in [1,5] using the definitions in 'score_definitions'. Interpret descriptions precisely and grade deterministically. 1 is the lowest and 5 is the highest\\n\"\n",
    "        \"4. Add a latency adjustment: latency_score = 1 / (1 + latency_sec/2), clipped to [0,1]\"\n",
    "        \"5. Round all numeric scores to 2 decimals\"\n",
    "\n",
    "        \"Scoring Details:\\n\"\n",
    "        \"- For each result:\\n\"\n",
    "        \"  • Analyze its results (title, url, price if given) to infer how well each criterion is met.\\n\"\n",
    "        \"  • If results are missing or malformed, score conservatively (1 or 2).\\n\"\n",
    "        \"  • Be consistent and deterministic.\\n\\n\"\n",
    "\n",
    "        \"Rules & Edge Cases:\\n\"\n",
    "        \"1) Use ONLY the rubric_json and the API results. Do NOT infer external data.\\n\"\n",
    "        \"2) If query not found in rubric_json, return an empty per_api list and winner='None'.\\n\"\n",
    "        \"3) 'notes' must summarize key strengths or weaknesses from the rubric perspective.\\n\"\n",
    "    ),\n",
    "    tools=[],  # pure rubric-based judging\n",
    "    output_type=AgentOutputSchema(JudgeReport, strict_json_schema=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77337b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "\n",
    "with open(\"evals/product_search_rubric.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        rubric_data = json.load(f)\n",
    "\n",
    "evals = []\n",
    "for item in rubric_data.get(\"rubrics\", []):\n",
    "    evals.append({\n",
    "        \"query\": item.get(\"query\"),\n",
    "        \"criteria\": item.get(\"criteria\")\n",
    "    })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3875d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: 'Black couch under 100'\n",
      "✓ EXA completed in 1.91s\n",
      "✓ PWS completed in 11.07s\n",
      "================================================================================\n",
      "EVALUATING RESULTS IN PARALLEL\n",
      "================================================================================\n",
      "Running 13 evaluations in parallel...\n",
      "✓ Completed 13 evaluations\n",
      "✓ Created EXA report: exa_evaluation_report_Black_couch_under_100.xlsx (10 results)\n",
      "✓ Created PWS report: pws_evaluation_report_Black_couch_under_100.xlsx (3 results)\n",
      "Searching for: 'Wooden dining table for 6 people'\n",
      "✓ EXA completed in 2.33s\n",
      "✓ PWS completed in 25.57s\n",
      "================================================================================\n",
      "EVALUATING RESULTS IN PARALLEL\n",
      "================================================================================\n",
      "Running 17 evaluations in parallel...\n",
      "✓ Completed 17 evaluations\n",
      "✓ Created EXA report: exa_evaluation_report_Wooden_dining_table_for_6_people.xlsx (10 results)\n",
      "✓ Created PWS report: pws_evaluation_report_Wooden_dining_table_for_6_people.xlsx (7 results)\n",
      "Searching for: 'White sneakers size 9 under $80'\n",
      "✓ EXA completed in 1.84s\n",
      "✓ PWS completed in 25.92s\n",
      "================================================================================\n",
      "EVALUATING RESULTS IN PARALLEL\n",
      "================================================================================\n",
      "Running 15 evaluations in parallel...\n",
      "✓ Completed 15 evaluations\n",
      "✓ Created EXA report: exa_evaluation_report_White_sneakers_size_9_under_$80.xlsx (10 results)\n",
      "✓ Created PWS report: pws_evaluation_report_White_sneakers_size_9_under_$80.xlsx (5 results)\n",
      "Searching for: 'Silk dresses for summer wedding'\n",
      "✓ EXA completed in 1.89s\n",
      "✓ PWS completed in 39.13s\n",
      "================================================================================\n",
      "EVALUATING RESULTS IN PARALLEL\n",
      "================================================================================\n",
      "Running 20 evaluations in parallel...\n",
      "✓ Completed 20 evaluations\n",
      "✓ Created EXA report: exa_evaluation_report_Silk_dresses_for_summer_wedding.xlsx (10 results)\n",
      "✓ Created PWS report: pws_evaluation_report_Silk_dresses_for_summer_wedding.xlsx (10 results)\n",
      "Searching for: 'Noise cancelling wireless headphones Sony'\n",
      "✓ EXA completed in 1.77s\n",
      "✓ PWS completed in 128.71s\n",
      "================================================================================\n",
      "EVALUATING RESULTS IN PARALLEL\n",
      "================================================================================\n",
      "Running 16 evaluations in parallel...\n",
      "✓ Completed 16 evaluations\n",
      "✓ Created EXA report: exa_evaluation_report_Noise_cancelling_wireless_headphones_Sony.xlsx (10 results)\n",
      "✓ Created PWS report: pws_evaluation_report_Noise_cancelling_wireless_headphones_Sony.xlsx (6 results)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import asyncio\n",
    "from agents.run import Runner\n",
    "import pandas as pd\n",
    "import sys\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "async def run_search(script_name, query):\n",
    "    \"\"\"Run a search script and return results with timing\"\"\"\n",
    "    start_time = time.time()\n",
    "    script_path = f\"/Users/karthikapurushothaman/projects/search/{script_name}\"\n",
    "    process = await asyncio.create_subprocess_exec(\n",
    "        \"/Users/karthikapurushothaman/projects/search/.venv/bin/python\",\n",
    "        script_path, \"-q\", query, \"-f\", \"json\",\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE\n",
    "    )\n",
    "    stdout, stderr = await process.communicate()\n",
    "    latency = time.time() - start_time\n",
    "    results = json.loads(stdout.decode())\n",
    "    return results, latency\n",
    "\n",
    "\n",
    "async def evaluate_result(api_name, query, idx, item, latency, rubric_data):\n",
    "    \"\"\"Evaluate a single result using the judge agent\"\"\"\n",
    "    formatted = [{\n",
    "        'title': item.get('title', ''),\n",
    "        'url': item.get('url', ''),\n",
    "        'price': item.get('price')\n",
    "    }]\n",
    "    \n",
    "    payload = {\n",
    "        'query': query,\n",
    "        'apis': [{\n",
    "            'name': api_name,\n",
    "            'latency_sec': latency,\n",
    "            'results': formatted\n",
    "        }],\n",
    "        'rubric_json': rubric_data\n",
    "    }\n",
    "    \n",
    "    result = await Runner.run(judge_agent, _msg(payload))\n",
    "    judge_output = result.final_output_as(JudgeReport)\n",
    "    return (api_name, query, idx, item, judge_output)\n",
    "\n",
    "\n",
    "for eval_type in evals:\n",
    "    query = eval_type[\"query\"]\n",
    "    evaluation_criteria = eval_type[\"criteria\"]\n",
    "\n",
    "    print(f\"Searching for: '{query}'\")\n",
    "    \n",
    "    # Run both searches in parallel\n",
    "    exa_task = run_search(\"exa_search.py\", query)\n",
    "    pws_task = run_search(\"pws_search.py\", query)\n",
    "    \n",
    "    (exa_results, exa_latency), (pws_results, pws_search) = await asyncio.gather(exa_task, pws_task)\n",
    "    \n",
    "    print(f\"✓ EXA completed in {exa_latency:.2f}s\")\n",
    "    print(f\"✓ PWS completed in {pws_search:.2f}s\")\n",
    "\n",
    "    # Prepare all evaluation tasks (both EXA and PWS in parallel)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EVALUATING RESULTS IN PARALLEL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    eval_tasks = []\n",
    "    \n",
    "    # Add EXA evaluation tasks\n",
    "    for idx, item in enumerate(exa_results.get('all_results', [])[:10], 1):\n",
    "        task = evaluate_result('EXA', query, idx, item, exa_latency, rubric_data)\n",
    "        eval_tasks.append(task)\n",
    "    \n",
    "    # Add PWS evaluation tasks\n",
    "    for idx, item in enumerate(pws_results.get('output', {}).get('matched_products', [])[:10], 1):\n",
    "        task = evaluate_result('PWS', query, idx, item, pws_search, rubric_data)\n",
    "        eval_tasks.append(task)\n",
    "    \n",
    "    # Run all evaluations in parallel\n",
    "    print(f\"Running {len(eval_tasks)} evaluations in parallel...\")\n",
    "    all_evaluations = await asyncio.gather(*eval_tasks)\n",
    "    time.sleep(10)\n",
    "    print(f\"✓ Completed {len(all_evaluations)} evaluations\")\n",
    "    # Prepare EXA data for Excel\n",
    "    exa_data = []\n",
    "\n",
    "    for api, query, idx, item, judge_output in all_evaluations:\n",
    "        if api == 'EXA':\n",
    "            row = {\n",
    "                'Result #': idx,\n",
    "                'Title': item.get('title', 'N/A'),\n",
    "                'URL': item.get('url', 'N/A'),\n",
    "                'Price': item.get('price', 'N/A'),\n",
    "            }\n",
    "            # Add each criterion score and notes (sanitize for Excel)\n",
    "            for criterion in judge_output.criteria:\n",
    "                # Remove ALL illegal Excel characters including control chars\n",
    "                clean_name = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F≤≥:()]', '', criterion.criteria_name).strip()\n",
    "                row[f\"{clean_name} - Score\"] = criterion.score\n",
    "                row[f\"{clean_name} - Reasoning\"] = criterion.notes\n",
    "            \n",
    "            exa_data.append(row)\n",
    "\n",
    "    # Prepare PWS data for Excel\n",
    "    pws_data = []\n",
    "    for api, query, idx, item, judge_output in all_evaluations:\n",
    "        if api == 'PWS':\n",
    "            row = {\n",
    "                'Result #': idx,\n",
    "                'Title': item.get('title', 'N/A'),\n",
    "                'URL': item.get('url', 'N/A'),\n",
    "                'Price': item.get('price', 'N/A'),\n",
    "            }\n",
    "            # Add each criterion score and notes (sanitize for Excel)\n",
    "            for criterion in judge_output.criteria:\n",
    "                # Remove ALL illegal Excel characters including control chars\n",
    "                clean_name = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F≤≥:()]', '', criterion.criteria_name).strip()\n",
    "                row[f\"{clean_name} - Score\"] = criterion.score\n",
    "                row[f\"{clean_name} - Reasoning\"] = criterion.notes\n",
    "            \n",
    "            pws_data.append(row)\n",
    "\n",
    "    # Create DataFrames\n",
    "    exa_df = pd.DataFrame(exa_data)\n",
    "    pws_df = pd.DataFrame(pws_data)\n",
    "\n",
    "    # Save to Excel files\n",
    "    exa_filename = f'exa_evaluation_report_{query.replace(\" \", \"_\")}.xlsx'\n",
    "    pws_filename = f'pws_evaluation_report_{query.replace(\" \", \"_\")}.xlsx'\n",
    "\n",
    "    exa_df.to_excel(exa_filename, index=False, engine='openpyxl')\n",
    "    pws_df.to_excel(pws_filename, index=False, engine='openpyxl')\n",
    "\n",
    "    print(f\"Created EXA report: {exa_filename} ({len(exa_df)} results)\")\n",
    "    print(f\"Created PWS report: {pws_filename} ({len(pws_df)} results)\")\n",
    "\n",
    "            \n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa43c38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (exa_venv)",
   "language": "python",
   "name": "exa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
