{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ddfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from agents import Agent, AgentOutputSchema \n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27423447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def _msg(payload: dict) -> list:\n",
    "    return [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"input_text\", \"text\": json.dumps(payload)}]\n",
    "    }]\n",
    "\n",
    "class JudgeReport(BaseModel):\n",
    "    ndgc: float                 \n",
    "    product_page_coverage: float             \n",
    "    domain_diversity: float   \n",
    "    latency: float\n",
    "    judge_notes: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51d9316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def _msg(payload: dict) -> list:\n",
    "    return [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"input_text\", \"text\": json.dumps(payload)}]\n",
    "    }]\n",
    "\n",
    "class Criteria(BaseModel):\n",
    "    criteria_name: str \n",
    "    score: float\n",
    "    notes: str \n",
    "\n",
    "class JudgeReport(BaseModel):\n",
    "    query: str\n",
    "    criteria: List[Criteria]\n",
    "\n",
    "\n",
    "judge_agent = Agent(\n",
    "    name=\"ProductSearchQualityJudge\",\n",
    "    instructions=(\n",
    "        \"You are an objective evaluator for shopping/search APIs.\\n\"\n",
    "        \"Do NOT browse the web. Use only the inputs provided.\\n\\n\"\n",
    "\n",
    "        \"Inputs:\\n\"\n",
    "        \"- query: the user's search query.\\n\"\n",
    "        \"- apis: array of { name, latency_sec (float), results: [ {title, url, price?} ] } per API.\\n\"\n",
    "        \"- rubric_json: a JSON rubric defining detailed evaluation criteria for multiple queries.\\n\\n\"\n",
    "\n",
    "        \"Your job:\\n\"\n",
    "        \"1. Identify the rubric in rubric_json whose 'query' best matches the given query (case-insensitive exact match preferred).\\n\"\n",
    "        \"2. For that rubric, evaluate each API’s results across all five criteria listed under 'criteria'.\\n\"\n",
    "        \"3. For each criterion, assign a numeric score in [1,5] using the definitions in 'score_definitions'. Interpret descriptions precisely and grade deterministically. 1 is the lowest and 5 is the highest\\n\"\n",
    "        \"4. Add a latency adjustment: latency_score = 1 / (1 + latency_sec/2), clipped to [0,1]\"\n",
    "        \"5. Round all numeric scores to 2 decimals\"\n",
    "\n",
    "        \"Scoring Details:\\n\"\n",
    "        \"- For each result:\\n\"\n",
    "        \"  • Analyze its results (title, url, price if given) to infer how well each criterion is met.\\n\"\n",
    "        \"  • If results are missing or malformed, score conservatively (1 or 2).\\n\"\n",
    "        \"  • Be consistent and deterministic.\\n\\n\"\n",
    "\n",
    "        \"Rules & Edge Cases:\\n\"\n",
    "        \"1) Use ONLY the rubric_json and the API results. Do NOT infer external data.\\n\"\n",
    "        \"2) If query not found in rubric_json, return an empty per_api list and winner='None'.\\n\"\n",
    "        \"3) 'notes' must summarize key strengths or weaknesses from the rubric perspective.\\n\"\n",
    "    ),\n",
    "    tools=[],  # pure rubric-based judging\n",
    "    output_type=AgentOutputSchema(JudgeReport, strict_json_schema=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77337b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "\n",
    "with open(\"evals/product_search_rubric.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        rubric_data = json.load(f)\n",
    "\n",
    "evals = []\n",
    "for item in rubric_data.get(\"rubrics\", []):\n",
    "    evals.append({\n",
    "        \"query\": item.get(\"query\"),\n",
    "        \"criteria\": item.get(\"criteria\")\n",
    "    })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3875d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: 'White sneakers size 9 under $80'\n",
      "✓ EXA completed in 2.32s\n",
      "✓ PWS completed in 164.48s\n",
      "================================================================================\n",
      "EVALUATING RESULTS IN PARALLEL\n",
      "================================================================================\n",
      "Running 20 evaluations in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-NxigQYNBYjmsIWgVCFZsCYQz on tokens per min (TPM): Limit 30000, Used 30000, Requested 3098. Please try again in 6.196s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. (request_id: req_4ef6903af49f4d5fb1f770989a9c65cb)\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-NxigQYNBYjmsIWgVCFZsCYQz on tokens per min (TPM): Limit 30000, Used 30000, Requested 3098. Please try again in 6.196s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Run all evaluations in parallel\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(eval_tasks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m evaluations in parallel...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m all_evaluations = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*eval_tasks)\n\u001b[32m     80\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Completed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_evaluations)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m evaluations\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mevaluate_result\u001b[39m\u001b[34m(api_name, query, idx, item, latency, rubric_data)\u001b[39m\n\u001b[32m     24\u001b[39m formatted = [{\n\u001b[32m     25\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m: item.get(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     26\u001b[39m     \u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m: item.get(\u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     27\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprice\u001b[39m\u001b[33m'\u001b[39m: item.get(\u001b[33m'\u001b[39m\u001b[33mprice\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     28\u001b[39m }]\n\u001b[32m     30\u001b[39m payload = {\n\u001b[32m     31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m: query,\n\u001b[32m     32\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mapis\u001b[39m\u001b[33m'\u001b[39m: [{\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrubric_json\u001b[39m\u001b[33m'\u001b[39m: rubric_data\n\u001b[32m     38\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(judge_agent, _msg(payload))\n\u001b[32m     41\u001b[39m judge_output = result.final_output_as(JudgeReport)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (api_name, query, idx, item, judge_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/exa/.venv/lib/python3.12/site-packages/agents/run.py:296\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, conversation_id, session)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mRun a workflow starting at the given agent.\u001b[39;00m\n\u001b[32m    249\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    292\u001b[39m \u001b[33;03m    type of the output.\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    295\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    297\u001b[39m     starting_agent,\n\u001b[32m    298\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    299\u001b[39m     context=context,\n\u001b[32m    300\u001b[39m     max_turns=max_turns,\n\u001b[32m    301\u001b[39m     hooks=hooks,\n\u001b[32m    302\u001b[39m     run_config=run_config,\n\u001b[32m    303\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    304\u001b[39m     conversation_id=conversation_id,\n\u001b[32m    305\u001b[39m     session=session,\n\u001b[32m    306\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/exa/.venv/lib/python3.12/site-packages/agents/run.py:548\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    543\u001b[39m logger.debug(\n\u001b[32m    544\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    545\u001b[39m )\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    549\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    550\u001b[39m             starting_agent,\n\u001b[32m    551\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    552\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    553\u001b[39m             _copy_str_or_list(prepared_input),\n\u001b[32m    554\u001b[39m             context_wrapper,\n\u001b[32m    555\u001b[39m         ),\n\u001b[32m    556\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    557\u001b[39m             agent=current_agent,\n\u001b[32m    558\u001b[39m             all_tools=all_tools,\n\u001b[32m    559\u001b[39m             original_input=original_input,\n\u001b[32m    560\u001b[39m             generated_items=generated_items,\n\u001b[32m    561\u001b[39m             hooks=hooks,\n\u001b[32m    562\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    563\u001b[39m             run_config=run_config,\n\u001b[32m    564\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    565\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    566\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    567\u001b[39m             conversation_id=conversation_id,\n\u001b[32m    568\u001b[39m         ),\n\u001b[32m    569\u001b[39m     )\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    571\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    572\u001b[39m         agent=current_agent,\n\u001b[32m    573\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    582\u001b[39m         conversation_id=conversation_id,\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/exa/.venv/lib/python3.12/site-packages/agents/run.py:1246\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id, conversation_id)\u001b[39m\n\u001b[32m   1243\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m   1244\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m   1247\u001b[39m     agent,\n\u001b[32m   1248\u001b[39m     system_prompt,\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1250\u001b[39m     output_schema,\n\u001b[32m   1251\u001b[39m     all_tools,\n\u001b[32m   1252\u001b[39m     handoffs,\n\u001b[32m   1253\u001b[39m     hooks,\n\u001b[32m   1254\u001b[39m     context_wrapper,\n\u001b[32m   1255\u001b[39m     run_config,\n\u001b[32m   1256\u001b[39m     tool_use_tracker,\n\u001b[32m   1257\u001b[39m     previous_response_id,\n\u001b[32m   1258\u001b[39m     conversation_id,\n\u001b[32m   1259\u001b[39m     prompt_config,\n\u001b[32m   1260\u001b[39m )\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m   1263\u001b[39m     agent=agent,\n\u001b[32m   1264\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1273\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m   1274\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/exa/.venv/lib/python3.12/site-packages/agents/run.py:1494\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, hooks, context_wrapper, run_config, tool_use_tracker, previous_response_id, conversation_id, prompt_config)\u001b[39m\n\u001b[32m   1479\u001b[39m \u001b[38;5;66;03m# If we have run hooks, or if the agent has hooks, we need to call them before the LLM call\u001b[39;00m\n\u001b[32m   1480\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1481\u001b[39m     hooks.on_llm_start(context_wrapper, agent, filtered.instructions, filtered.input),\n\u001b[32m   1482\u001b[39m     (\n\u001b[32m   (...)\u001b[39m\u001b[32m   1491\u001b[39m     ),\n\u001b[32m   1492\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1494\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1495\u001b[39m     system_instructions=filtered.instructions,\n\u001b[32m   1496\u001b[39m     \u001b[38;5;28minput\u001b[39m=filtered.input,\n\u001b[32m   1497\u001b[39m     model_settings=model_settings,\n\u001b[32m   1498\u001b[39m     tools=all_tools,\n\u001b[32m   1499\u001b[39m     output_schema=output_schema,\n\u001b[32m   1500\u001b[39m     handoffs=handoffs,\n\u001b[32m   1501\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1502\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1503\u001b[39m     ),\n\u001b[32m   1504\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1505\u001b[39m     conversation_id=conversation_id,\n\u001b[32m   1506\u001b[39m     prompt=prompt_config,\n\u001b[32m   1507\u001b[39m )\n\u001b[32m   1509\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m   1511\u001b[39m \u001b[38;5;66;03m# If we have run hooks, or if the agent has hooks, we need to call them after the LLM call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/exa/.venv/lib/python3.12/site-packages/agents/models/openai_responses.py:90\u001b[39m, in \u001b[36mOpenAIResponsesModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, conversation_id, prompt)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m response_span(disabled=tracing.is_disabled()) \u001b[38;5;28;01mas\u001b[39;00m span_response:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     91\u001b[39m             system_instructions,\n\u001b[32m     92\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     93\u001b[39m             model_settings,\n\u001b[32m     94\u001b[39m             tools,\n\u001b[32m     95\u001b[39m             output_schema,\n\u001b[32m     96\u001b[39m             handoffs,\n\u001b[32m     97\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m     98\u001b[39m             conversation_id=conversation_id,\n\u001b[32m     99\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    100\u001b[39m             prompt=prompt,\n\u001b[32m    101\u001b[39m         )\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001b[32m    104\u001b[39m             logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLLM responded\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/exa/.venv/lib/python3.12/site-packages/agents/models/openai_responses.py:305\u001b[39m, in \u001b[36mOpenAIResponsesModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, previous_response_id, conversation_id, stream, prompt)\u001b[39m\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    303\u001b[39m         response_format = {\u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: model_settings.verbosity}\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.responses.create(\n\u001b[32m    306\u001b[39m     previous_response_id=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(previous_response_id),\n\u001b[32m    307\u001b[39m     conversation=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(conversation_id),\n\u001b[32m    308\u001b[39m     instructions=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(system_instructions),\n\u001b[32m    309\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    310\u001b[39m     \u001b[38;5;28minput\u001b[39m=list_input,\n\u001b[32m    311\u001b[39m     include=include,\n\u001b[32m    312\u001b[39m     tools=converted_tools_payload,\n\u001b[32m    313\u001b[39m     prompt=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(prompt),\n\u001b[32m    314\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    315\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    316\u001b[39m     truncation=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.truncation),\n\u001b[32m    317\u001b[39m     max_output_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    318\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    319\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    320\u001b[39m     stream=stream,\n\u001b[32m    321\u001b[39m     extra_headers=\u001b[38;5;28mself\u001b[39m._merge_headers(model_settings),\n\u001b[32m    322\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    323\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    324\u001b[39m     text=response_format,\n\u001b[32m    325\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.store),\n\u001b[32m    326\u001b[39m     reasoning=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.reasoning),\n\u001b[32m    327\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    328\u001b[39m     **extra_args,\n\u001b[32m    329\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/exa/.venv/lib/python3.12/site-packages/openai/resources/responses/responses.py:2259\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2222\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2223\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2224\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2257\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2258\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m2259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2260\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2261\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2262\u001b[39m             {\n\u001b[32m   2263\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbackground\u001b[39m\u001b[33m\"\u001b[39m: background,\n\u001b[32m   2264\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mconversation\u001b[39m\u001b[33m\"\u001b[39m: conversation,\n\u001b[32m   2265\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   2266\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2267\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   2268\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   2269\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: max_tool_calls,\n\u001b[32m   2270\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2271\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2272\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2273\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   2274\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m   2275\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2276\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   2277\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2278\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2279\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2280\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2281\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2282\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2283\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   2284\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2285\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2286\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2287\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2288\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   2289\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2290\u001b[39m             },\n\u001b[32m   2291\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   2292\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2293\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   2294\u001b[39m         ),\n\u001b[32m   2295\u001b[39m         options=make_request_options(\n\u001b[32m   2296\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2297\u001b[39m         ),\n\u001b[32m   2298\u001b[39m         cast_to=Response,\n\u001b[32m   2299\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2300\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   2301\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/exa/.venv/lib/python3.12/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/exa/.venv/lib/python3.12/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1591\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1593\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-NxigQYNBYjmsIWgVCFZsCYQz on tokens per min (TPM): Limit 30000, Used 30000, Requested 3098. Please try again in 6.196s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import asyncio\n",
    "from agents.run import Runner\n",
    "\n",
    "\n",
    "async def run_search(script_name, query):\n",
    "    \"\"\"Run a search script and return results with timing\"\"\"\n",
    "    start_time = time.time()\n",
    "    script_path = f\"/Users/karthikapurushothaman/projects/search/{script_name}\"\n",
    "    process = await asyncio.create_subprocess_exec(\n",
    "        \"/Users/karthikapurushothaman/projects/search/.venv/bin/python\",\n",
    "        script_path, \"-q\", query, \"-f\", \"json\",\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE\n",
    "    )\n",
    "    stdout, stderr = await process.communicate()\n",
    "    latency = time.time() - start_time\n",
    "    results = json.loads(stdout.decode())\n",
    "    return results, latency\n",
    "\n",
    "\n",
    "async def evaluate_result(api_name, query, idx, item, latency, rubric_data):\n",
    "    \"\"\"Evaluate a single result using the judge agent\"\"\"\n",
    "    formatted = [{\n",
    "        'title': item.get('title', ''),\n",
    "        'url': item.get('url', ''),\n",
    "        'price': item.get('price')\n",
    "    }]\n",
    "    \n",
    "    payload = {\n",
    "        'query': query,\n",
    "        'apis': [{\n",
    "            'name': api_name,\n",
    "            'latency_sec': latency,\n",
    "            'results': formatted\n",
    "        }],\n",
    "        'rubric_json': rubric_data\n",
    "    }\n",
    "    \n",
    "    result = await Runner.run(judge_agent, _msg(payload))\n",
    "    judge_output = result.final_output_as(JudgeReport)\n",
    "    return (api_name, query, idx, item, judge_output)\n",
    "\n",
    "\n",
    "for eval_type in evals[4]:\n",
    "    query = eval_type[\"query\"]\n",
    "    evaluation_criteria = eval_type[\"criteria\"]\n",
    "\n",
    "    print(f\"Searching for: '{query}'\")\n",
    "    \n",
    "    # Run both searches in parallel\n",
    "    exa_task = run_search(\"exa_search.py\", query)\n",
    "    pws_task = run_search(\"pws_search.py\", query)\n",
    "    \n",
    "    (exa_results, exa_latency), (pws_results, pws_search) = await asyncio.gather(exa_task, pws_task)\n",
    "    \n",
    "    print(f\"✓ EXA completed in {exa_latency:.2f}s\")\n",
    "    print(f\"✓ PWS completed in {pws_search:.2f}s\")\n",
    "\n",
    "    # Prepare all evaluation tasks (both EXA and PWS in parallel)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EVALUATING RESULTS IN PARALLEL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    eval_tasks = []\n",
    "    \n",
    "    # Add EXA evaluation tasks\n",
    "    for idx, item in enumerate(exa_results.get('all_results', [])[:10], 1):\n",
    "        task = evaluate_result('EXA', query, idx, item, exa_latency, rubric_data)\n",
    "        eval_tasks.append(task)\n",
    "    \n",
    "    # Add PWS evaluation tasks\n",
    "    for idx, item in enumerate(pws_results.get('output', {}).get('matched_products', [])[:10], 1):\n",
    "        task = evaluate_result('PWS', query, idx, item, pws_search, rubric_data)\n",
    "        eval_tasks.append(task)\n",
    "    \n",
    "    # Run all evaluations in parallel\n",
    "    print(f\"Running {len(eval_tasks)} evaluations in parallel...\")\n",
    "    all_evaluations = await asyncio.gather(*eval_tasks)\n",
    "    print(f\"✓ Completed {len(all_evaluations)} evaluations\")\n",
    "            \n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e551b95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "# Install required packages\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\", \"openpyxl\", \"-q\"])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare EXA data for Excel\n",
    "exa_data = []\n",
    "for api, idx, item, judge_output in all_evaluations:\n",
    "    if api == 'EXA':\n",
    "        row = {\n",
    "            'Result #': idx,\n",
    "            'Title': item.get('title', 'N/A'),\n",
    "            'URL': item.get('url', 'N/A'),\n",
    "            'Price': item.get('price', 'N/A'),\n",
    "        }\n",
    "        # Add each criterion score and notes (sanitize for Excel)\n",
    "        for criterion in judge_output.criteria:\n",
    "            # Remove ALL illegal Excel characters including control chars\n",
    "            clean_name = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F≤≥:()]', '', criterion.criteria_name).strip()\n",
    "            row[f\"{clean_name} - Score\"] = criterion.score\n",
    "            row[f\"{clean_name} - Reasoning\"] = criterion.notes\n",
    "        \n",
    "        exa_data.append(row)\n",
    "\n",
    "# Prepare PWS data for Excel\n",
    "pws_data = []\n",
    "for api, idx, item, judge_output in all_evaluations:\n",
    "    if api == 'PWS':\n",
    "        row = {\n",
    "            'Result #': idx,\n",
    "            'Title': item.get('title', 'N/A'),\n",
    "            'URL': item.get('url', 'N/A'),\n",
    "            'Price': item.get('price', 'N/A'),\n",
    "        }\n",
    "        # Add each criterion score and notes (sanitize for Excel)\n",
    "        for criterion in judge_output.criteria:\n",
    "            # Remove ALL illegal Excel characters including control chars\n",
    "            clean_name = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F≤≥:()]', '', criterion.criteria_name).strip()\n",
    "            row[f\"{clean_name} - Score\"] = criterion.score\n",
    "            row[f\"{clean_name} - Reasoning\"] = criterion.notes\n",
    "        \n",
    "        pws_data.append(row)\n",
    "\n",
    "# Create DataFrames\n",
    "exa_df = pd.DataFrame(exa_data)\n",
    "pws_df = pd.DataFrame(pws_data)\n",
    "\n",
    "# Save to Excel files\n",
    "exa_filename = f'exa_evaluation_report_{query.replace(\" \", \"_\")}.xlsx'\n",
    "pws_filename = f'pws_evaluation_report_{query.replace(\" \", \"_\")}.xlsx'\n",
    "\n",
    "exa_df.to_excel(exa_filename, index=False, engine='openpyxl')\n",
    "pws_df.to_excel(pws_filename, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"✓ Created EXA report: {exa_filename} ({len(exa_df)} results)\")\n",
    "print(f\"✓ Created PWS report: {pws_filename} ({len(pws_df)} results)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa43c38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (exa_venv)",
   "language": "python",
   "name": "exa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
