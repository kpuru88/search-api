{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22b2053",
   "metadata": {},
   "source": [
    "# Search API Relevance Judge\n",
    "\n",
    "This notebook evaluates and compares two search APIs (EXA and PWS) using AI-powered judging.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Parallel Search Execution**: Run both APIs simultaneously for speed\n",
    "2. **AI-Powered Evaluation**: Use OpenAI agents to judge each result against custom rubrics\n",
    "3. **Criteria-Based Scoring**: Evaluate multiple criteria (relevance, price fit, product match, etc.)\n",
    "4. **Excel Reports**: Generate detailed evaluation reports with scores and reasoning\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import all required dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ddfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from agents import Agent, AgentOutputSchema \n",
    "import time\n",
    "import asyncio\n",
    "from agents.run import Runner\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8743b",
   "metadata": {},
   "source": [
    "## Define AI Judge Agent\n",
    "\n",
    "Configure the LLM agent that will evaluate search results based on custom rubrics.\n",
    "\n",
    "**Key Components:**\n",
    "- `Criteria`: Schema for individual criterion scores (name, score, notes)\n",
    "- `JudgeReport`: Overall evaluation schema (query, criteria list)\n",
    "- `judge_agent`: The AI agent with instructions for objective evaluation\n",
    "\n",
    "The agent uses GPT-5 to score each result on a 1-5 scale across multiple criteria defined in the rubric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def _msg(payload: dict) -> list:\n",
    "    return [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"input_text\", \"text\": json.dumps(payload)}]\n",
    "    }]\n",
    "\n",
    "class Criteria(BaseModel):\n",
    "    criteria_name: str \n",
    "    score: float\n",
    "    notes: str \n",
    "\n",
    "class JudgeReport(BaseModel):\n",
    "    query: str\n",
    "    criteria: List[Criteria]\n",
    "\n",
    "judge_agent = Agent(\n",
    "    name=\"ProductSearchQualityJudge\",\n",
    "    model=\"gpt-5\", \n",
    "    instructions=(\n",
    "        \"You are an objective evaluator for shopping/search APIs.\\n\"\n",
    "        \"Do NOT browse the web. Use only the inputs provided.\\n\\n\"\n",
    "\n",
    "        \"Inputs:\\n\"\n",
    "        \"- query: the user's search query.\\n\"\n",
    "        \"- apis: array of { name, latency_sec (float), results: [ {title, url, price?} ] } per API.\\n\"\n",
    "        \"- rubric_json: a JSON rubric defining detailed evaluation criteria for multiple queries.\\n\\n\"\n",
    "\n",
    "        \"Your job:\\n\"\n",
    "        \"1. Identify the rubric in rubric_json whose 'query' best matches the given query (case-insensitive exact match preferred).\\n\"\n",
    "        \"2. For that rubric, evaluate each APIâ€™s results across all five criteria listed under 'criteria'.\\n\"\n",
    "        \"3. For each criterion, assign a numeric score in [1,5] using the definitions in 'score_definitions'. Interpret descriptions precisely and grade deterministically. 1 is the lowest and 5 is the highest\\n\"\n",
    "        \"4. Add a latency adjustment: latency_score = 1 / (1 + latency_sec/2), clipped to [0,1]\"\n",
    "        \"5. Round all numeric scores to 2 decimals\"\n",
    "\n",
    "        \"Scoring Details:\\n\"\n",
    "        \"- For each result:\\n\"\n",
    "        \"  â€¢ Analyze its results (title, url, price if given) to infer how well each criterion is met.\\n\"\n",
    "        \"  â€¢ If results are missing or malformed, score conservatively (1 or 2).\\n\"\n",
    "        \"  â€¢ Be consistent and deterministic.\\n\\n\"\n",
    "\n",
    "        \"Rules & Edge Cases:\\n\"\n",
    "        \"1) Use ONLY the rubric_json and the API results. Do NOT infer external data.\\n\"\n",
    "        \"2) If query not found in rubric_json, return an empty per_api list and winner='None'.\\n\"\n",
    "        \"3) 'notes' must summarize key strengths or weaknesses from the rubric perspective.\\n\"\n",
    "    ),\n",
    "    tools=[],  # pure rubric-based judging\n",
    "    output_type=AgentOutputSchema(JudgeReport, strict_json_schema=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1866cf",
   "metadata": {},
   "source": [
    "## Load Evaluation Rubrics\n",
    "\n",
    "Load the evaluation criteria from `evals/product_search_rubric.json`.\n",
    "\n",
    "Each rubric defines:\n",
    "- **Query**: The search query to evaluate\n",
    "- **Criteria**: Multiple scoring dimensions (color match, price fit, relevance, etc.)\n",
    "- **Score Definitions**: What each score (1-5) means for each criterion\n",
    "\n",
    "The rubrics guide the AI judge on how to evaluate search results objectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77337b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"evals/product_search_rubric.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        rubric_data = json.load(f)\n",
    "\n",
    "evals = []\n",
    "for item in rubric_data.get(\"rubrics\", []):\n",
    "    evals.append({\n",
    "        \"query\": item.get(\"query\"),\n",
    "        \"criteria\": item.get(\"criteria\")\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dabc533",
   "metadata": {},
   "source": [
    "## Execute Evaluations\n",
    "\n",
    "This section contains the main evaluation logic:\n",
    "\n",
    "### Helper Functions\n",
    "\n",
    "**`run_search(script_name, query)`**\n",
    "- Executes a search script (EXA or PWS) asynchronously  \n",
    "- Returns: `(results, latency)` tuple\n",
    "- Enables parallel search execution with `asyncio.gather()`\n",
    "\n",
    "**`evaluate_result(api_name, query, idx, item, latency, rubric_data)`**\n",
    "- Sends a single search result to the AI judge for evaluation\n",
    "- Returns: Tuple of `(api_name, query, idx, item, judge_output)`\n",
    "- Enables parallel evaluation of all results\n",
    "\n",
    "### Main Execution Loop\n",
    "\n",
    "For each query in the rubric:\n",
    "1. **Parallel Search** - Run both EXA and PWS searches simultaneously\n",
    "2. **Prepare Tasks** - Create evaluation tasks for top 10 results from each API\n",
    "3. **Parallel Judging** - Send all results to AI judge concurrently (up to 20 evaluations)\n",
    "4. **Generate Excel** - Create separate reports for EXA and PWS with scores and reasoning\n",
    "\n",
    "**Performance:** All operations are parallelized for maximum speed!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3875d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_search(script_name, query):\n",
    "    \"\"\"Run a search script and return results with timing\"\"\"\n",
    "    start_time = time.time()\n",
    "    script_path = f\"/Users/karthikapurushothaman/projects/search/{script_name}\"\n",
    "    process = await asyncio.create_subprocess_exec(\n",
    "        \"/Users/karthikapurushothaman/projects/search/.venv/bin/python\",\n",
    "        script_path, \"-q\", query, \"-f\", \"json\",\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE\n",
    "    )\n",
    "    stdout, stderr = await process.communicate()\n",
    "    latency = time.time() - start_time\n",
    "    results = json.loads(stdout.decode())\n",
    "    return results, latency\n",
    "\n",
    "async def evaluate_result(api_name, query, idx, item, latency, rubric_data):\n",
    "    \"\"\"Evaluate a single result using the judge agent\"\"\"\n",
    "    formatted = [{\n",
    "        'title': item.get('title', ''),\n",
    "        'url': item.get('url', ''),\n",
    "        'price': item.get('price')\n",
    "    }]\n",
    "    \n",
    "    payload = {\n",
    "        'query': query,\n",
    "        'apis': [{\n",
    "            'name': api_name,\n",
    "            'latency_sec': latency,\n",
    "            'results': formatted\n",
    "        }],\n",
    "        'rubric_json': rubric_data\n",
    "    }\n",
    "    \n",
    "    result = await Runner.run(judge_agent, _msg(payload))\n",
    "    judge_output = result.final_output_as(JudgeReport)\n",
    "    return (api_name, query, idx, item, judge_output)\n",
    "\n",
    "for eval_type in evals:\n",
    "    query = eval_type[\"query\"]\n",
    "    evaluation_criteria = eval_type[\"criteria\"]\n",
    "\n",
    "    print(f\"Searching for: '{query}'\")\n",
    "    \n",
    "    exa_task = run_search(\"exa_search.py\", query)\n",
    "    pws_task = run_search(\"pws_search.py\", query)\n",
    "    \n",
    "    (exa_results, exa_latency), (pws_results, pws_search) = await asyncio.gather(exa_task, pws_task)\n",
    "    \n",
    "    print(f\"âœ“ EXA completed in {exa_latency:.2f}s\")\n",
    "    print(f\"âœ“ PWS completed in {pws_search:.2f}s\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EVALUATING RESULTS IN PARALLEL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    eval_tasks = []\n",
    "    \n",
    "    for idx, item in enumerate(exa_results.get('all_results', [])[:10], 1):\n",
    "        task = evaluate_result('EXA', query, idx, item, exa_latency, rubric_data)\n",
    "        eval_tasks.append(task)\n",
    "    \n",
    "    for idx, item in enumerate(pws_results.get('output', {}).get('matched_products', [])[:10], 1):\n",
    "        task = evaluate_result('PWS', query, idx, item, pws_search, rubric_data)\n",
    "        eval_tasks.append(task)\n",
    "    \n",
    "    print(f\"Running {len(eval_tasks)} evaluations in parallel...\")\n",
    "    all_evaluations = await asyncio.gather(*eval_tasks)\n",
    "    time.sleep(10)\n",
    "    print(f\"âœ“ Completed {len(all_evaluations)} evaluations\")\n",
    "    \n",
    "    exa_data = []\n",
    "    for api, query, idx, item, judge_output in all_evaluations:\n",
    "        if api == 'EXA':\n",
    "            row = {\n",
    "                'Result #': idx,\n",
    "                'Title': item.get('title', 'N/A'),\n",
    "                'URL': item.get('url', 'N/A'),\n",
    "                'Price': item.get('price', 'N/A'),\n",
    "            }\n",
    "            for criterion in judge_output.criteria:\n",
    "                clean_name = re.sub(r'[\\x00-\\x1F\\x7F-\\x9Fâ‰¤â‰¥:()]', '', criterion.criteria_name).strip()\n",
    "                row[f\"{clean_name} - Score\"] = criterion.score\n",
    "                row[f\"{clean_name} - Reasoning\"] = criterion.notes\n",
    "            \n",
    "            exa_data.append(row)\n",
    "\n",
    "    pws_data = []\n",
    "    for api, query, idx, item, judge_output in all_evaluations:\n",
    "        if api == 'PWS':\n",
    "            row = {\n",
    "                'Result #': idx,\n",
    "                'Title': item.get('title', 'N/A'),\n",
    "                'URL': item.get('url', 'N/A'),\n",
    "                'Price': item.get('price', 'N/A'),\n",
    "            }\n",
    "            for criterion in judge_output.criteria:\n",
    "                clean_name = re.sub(r'[\\x00-\\x1F\\x7F-\\x9Fâ‰¤â‰¥:()]', '', criterion.criteria_name).strip()\n",
    "                row[f\"{clean_name} - Score\"] = criterion.score\n",
    "                row[f\"{clean_name} - Reasoning\"] = criterion.notes\n",
    "            \n",
    "            pws_data.append(row)\n",
    "\n",
    "    exa_df = pd.DataFrame(exa_data)\n",
    "    pws_df = pd.DataFrame(pws_data)\n",
    "\n",
    "    exa_filename = f'exa_evaluation_report_{query.replace(\" \", \"_\")}.xlsx'\n",
    "    pws_filename = f'pws_evaluation_report_{query.replace(\" \", \"_\")}.xlsx'\n",
    "\n",
    "    exa_df.to_excel(exa_filename, index=False, engine='openpyxl')\n",
    "    pws_df.to_excel(pws_filename, index=False, engine='openpyxl')\n",
    "\n",
    "    print(f\"Created EXA report: {exa_filename} ({len(exa_df)} results)\")\n",
    "    print(f\"Created PWS report: {pws_filename} ({len(pws_df)} results)\")\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb6e619",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "After running the evaluations, you'll find:\n",
    "\n",
    "### Excel Reports\n",
    "- `exa_evaluation_report_{query}.xlsx` - EXA API results with detailed scoring\n",
    "- `pws_evaluation_report_{query}.xlsx` - PWS API results with detailed scoring\n",
    "\n",
    "### Report Contents\n",
    "Each Excel file includes:\n",
    "- **Result metadata**: Title, URL, Price\n",
    "- **Criterion scores**: Individual scores (1-5) for each evaluation criterion\n",
    "- **Judge reasoning**: Detailed notes explaining each score\n",
    "- **Comparison**: Easy side-by-side comparison of both APIs\n",
    "\n",
    "### Next Steps\n",
    "1. Open the Excel files to review detailed evaluations\n",
    "2. Compare scores across different criteria\n",
    "3. Identify which API performs better for specific query types\n",
    "4. Adjust rubrics in `evals/product_search_rubric.json` for different evaluation needs\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ’¡ **Tip**: Modify the rubric JSON to add new queries or change scoring criteria!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (exa_venv)",
   "language": "python",
   "name": "exa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
